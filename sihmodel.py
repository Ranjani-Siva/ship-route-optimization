# -*- coding: utf-8 -*-
"""sihmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11QQQj0k1MOZyX5xC1dDiNkbfeWFlFkB3
"""

!pip install statsmodels

# Import necessary libraries
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
import joblib
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

data =  pd.read_csv('/content/drive/MyDrive/sih2024/mergeddata.csv')

# 1. mlotst (Mixed Layer Ocean Temperature) - approx. based on latitude and SOG
data['mlotst'] = 15 + 0.1 * data['SOG'] - 0.05 * data['latitude_x']

# 2. thetao (Ocean Potential Temperature) - approx. based on latitude and draft
data['thetao'] = 10 + 0.2 * data['Draft'] - 0.01 * abs(data['latitude_x'])

# 3. bottomT (Bottom Temperature) - approx. based on draft
data['bottomT'] = 5 + 0.1 * data['Draft']

# 4. vo (Meridional Velocity) - based on COG and Heading
data['vo'] = data['SOG'] * np.sin(np.radians(data['COG']))

# 5. uo (Zonal Velocity) - based on COG and Heading
data['uo'] = data['SOG'] * np.cos(np.radians(data['COG']))

# 6. so (Sea Water Salinity) - approximated based on location (latitude)
data['so'] = 35 + 0.02 * abs(data['latitude_x'])

# 7. zos (Sea Surface Height) - random approximation for now
data['zos'] = 0.5 * np.random.uniform(-1, 1, size=len(data))

# 8. rel_dir (Relative Direction) - difference between COG and Heading
data['rel_dir'] = data['COG'] - data['Heading']

# 9. COG_true (True Course Over Ground) - directly from COG
data['COG_true'] = data['COG']

# 10. dir_4 (Categorical Direction) - simplifying heading into categories
def categorize_direction(heading):
    if 0 <= heading < 90:
        return 'N'
    elif 90 <= heading < 180:
        return 'E'
    elif 180 <= heading < 270:
        return 'S'
    else:
        return 'W'

data['dir_4'] = data['Heading'].apply(categorize_direction)

data.head()

def remove_outliers_iqr(df, column):
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  df_filtered = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
  return df_filtered

# Example: Remove outliers from 'column_name'
for column in data.columns:
  if pd.api.types.is_numeric_dtype(data[column]):
      data = remove_outliers_iqr(data, column)
data.head()

correlation_matrix = data.corr(numeric_only=True)

# Create a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

data.isnull().sum()

X = data.drop(['SOG','dir_4','BaseDateTime','Draft','VesselName','IMO','CallSign','TransceiverClass','time','MMSI','latitude_y','longitude_y','latitude_x','longitude_x'], axis=1)
y = data['Draft']
print(X.dtypes)

print("NaN values in each column of X:")
print(X.isna().sum())

# Check for infinite values
print("Infinite values in each column of X:")
print(X.select_dtypes(include=np.number).apply(np.isinf).sum())

X.dtypes

X.replace([np.inf, -np.inf], np.nan, inplace=True)

X.dropna(inplace=True)
y = y[X.index]

X.fillna(X.mean(), inplace=True)

print("Shape of X:", X.shape)
print("Shape of y:", y.shape)

# Check if X or y are empty
print("Is X empty?", X.empty if isinstance(X, pd.DataFrame) else len(X) == 0)
print("Is y empty?", y.empty if isinstance(y, pd.Series) else len(y) == 0)

print("Are the indices of X and y aligned?", X.index.equals(y.index))

# Check for zero variance columns
zero_variance_cols = X.columns[X.nunique() <= 1]
print("Zero variance columns:", zero_variance_cols)

# Drop zero variance columns if any
X = X.drop(columns=zero_variance_cols)

# Convert relevant columns to numeric types
for col in X.columns:
    if X[col].dtype == 'object':
        try:
            X[col] = pd.to_numeric(X[col])
        except ValueError:
            print(f"Could not convert column {col} to numeric. Please investigate this column.")

# Add constant and fit the OLS model
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()

# Print the summary
print(model.summary())

# Define the threshold for p-value
p_threshold = 0.05

# Add a constant (intercept) to the model
X = sm.add_constant(X)

# Perform backward elimination
while True:
    # Fit the model
    model = sm.OLS(y, X).fit()

    # Get p-values for all variables
    p_values = model.pvalues

    # Find the variable with the highest p-value
    max_p_value = p_values.max()

    # If the highest p-value is greater than the threshold, remove the variable
    if max_p_value > p_threshold:
        max_p_variable = p_values.idxmax()
        print(f"Removing {max_p_variable} with p-value {max_p_value}")
        X = X.drop(columns=[max_p_variable])  # Drop the variable with highest p-value
    else:
        break

# Display the final model summary
print(model.summary())

X.columns

from statsmodels.stats.outliers_influence import variance_inflation_factor


# Define the feature set without the constant for VIF calculation
#X_vif = data[['COG','Heading','VesselType','Status','Width','Draft','Cargo','VCMX','VHM0','VHM0_WW','VMDR','VMDR_WW','VTM01_WW','VTPK']]
X_vif = data[['COG', 'mlotst', 'thetao', 'bottomT', 'so', 'rel_dir','COG_true']]
# Create a DataFrame to store VIF values
vif_data = pd.DataFrame()
vif_data['Feature'] = X_vif.columns
vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]

# Display the VIF values
print(vif_data)





correlation_matrix = data[['VCMX','VHM0_WW']].corr()
print(correlation_matrix)

# If correlation is high, remove one of the variables
X_vif_reduced = X_vif.drop(columns=['VCMX'])

# Recalculate VIF after removing the variable
vif_data_reduced = pd.DataFrame()
vif_data_reduced['Feature'] = X_vif_reduced.columns
vif_data_reduced['VIF'] = [variance_inflation_factor(X_vif_reduced.values, i) for i in range(X_vif_reduced.shape[1])]

# Display the new VIF values
print(vif_data_reduced)

# If correlation is high, remove one of the variables
X_vif_reduced = X_vif.drop(columns=['VCMX','VMDR'])

# Recalculate VIF after removing the variable
vif_data_reduced = pd.DataFrame()
vif_data_reduced['Feature'] = X_vif_reduced.columns
vif_data_reduced['VIF'] = [variance_inflation_factor(X_vif_reduced.values, i) for i in range(X_vif_reduced.shape[1])]

# Display the new VIF values
print(vif_data_reduced)

# If correlation is high, remove one of the variables
X_vif_reduced = X_vif.drop(columns=['VCMX','VMDR','VTPK'])

# Recalculate VIF after removing the variable
vif_data_reduced = pd.DataFrame()
vif_data_reduced['Feature'] = X_vif_reduced.columns
vif_data_reduced['VIF'] = [variance_inflation_factor(X_vif_reduced.values, i) for i in range(X_vif_reduced.shape[1])]

# Display the new VIF values
print(vif_data_reduced)

# If correlation is high, remove one of the variables
X_vif_reduced = X_vif.drop(columns=['VCMX','VMDR','VTPK','VTM01_WW'])

# Recalculate VIF after removing the variable
vif_data_reduced = pd.DataFrame()
vif_data_reduced['Feature'] = X_vif_reduced.columns
vif_data_reduced['VIF'] = [variance_inflation_factor(X_vif_reduced.values, i) for i in range(X_vif_reduced.shape[1])]

# Display the new VIF values
print(vif_data_reduced)

# If correlation is high, remove one of the variables
X_vif_reduced = X_vif.drop(columns=['VCMX','VMDR','VTPK','VTM01_WW','VMDR_WW'])

# Recalculate VIF after removing the variable
vif_data_reduced = pd.DataFrame()
vif_data_reduced['Feature'] = X_vif_reduced.columns
vif_data_reduced['VIF'] = [variance_inflation_factor(X_vif_reduced.values, i) for i in range(X_vif_reduced.shape[1])]

# Display the new VIF values
print(vif_data_reduced)

# If correlation is high, remove one of the variables
X_vif_reduced = X_vif.drop(columns=['VCMX','VMDR','VTPK','VTM01_WW','VMDR_WW','VHM0'])

# Recalculate VIF after removing the variable
vif_data_reduced = pd.DataFrame()
vif_data_reduced['Feature'] = X_vif_reduced.columns
vif_data_reduced['VIF'] = [variance_inflation_factor(X_vif_reduced.values, i) for i in range(X_vif_reduced.shape[1])]

# Display the new VIF values
print(vif_data_reduced)

# If correlation is high, remove one of the variables
X_vif_reduced = X_vif.drop(columns=['VCMX','VMDR','VTPK','VTM01_WW','VMDR_WW','VHM0','VesselType'])

# Recalculate VIF after removing the variable
vif_data_reduced = pd.DataFrame()
vif_data_reduced['Feature'] = X_vif_reduced.columns
vif_data_reduced['VIF'] = [variance_inflation_factor(X_vif_reduced.values, i) for i in range(X_vif_reduced.shape[1])]

# Display the new VIF values
print(vif_data_reduced)

X = data[['COG','Heading','Status','Width','Draft','Cargo','VHM0_WW']]
# Check for missing data
print(X.isnull().sum())

# Define the feature set after removing
X = data[['COG','Heading','Status','Width','Draft','Cargo','VHM0_WW']]

# Add a constant to the model (intercept)
X = sm.add_constant(X)

# Define the dependent variable
y = data['SOG']

# Fit the OLS regression model
model = sm.OLS(y, X).fit()

# Print the summary of the regression model
print(model.summary())

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Rerun OLS with scaled features
model_scaled = sm.OLS(y, X_scaled).fit()
print(model_scaled.summary())

y_pred = model.predict(X)

# Create a scatter plot of actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y, y_pred)
plt.xlabel("Actual Fuel Consumption")
plt.ylabel("Predicted Fuel Consumption")
plt.title("Actual vs. Predicted Fuel Consumption")
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)  # Add a diagonal line for reference
plt.show()



# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Decision Tree Regressor
model = DecisionTreeRegressor(max_depth=5, random_state=42)
# Train the model
model.fit(X_train, y_train)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")



# Create and fit the Ridge regression model
ridge_model = Ridge(alpha=1.0)  # You can adjust the alpha value
ridge_model.fit(X_train, y_train)

# Predict on the test set
y_pred = ridge_model.predict(X_test)

# Evaluate the model
ridge_mse = mean_squared_error(y_test, y_pred)
ridge_r2 = r2_score(y_test, y_pred)

print("Ridge Regression MSE:", ridge_mse)
print("Ridge Regression R-squared:", ridge_r2)

# Create and fit the Lasso regression model
lasso_model = Lasso(alpha=0.1)  # You can adjust the alpha value
lasso_model.fit(X_train, y_train)

# Predict on the test set
y_pred_lasso = lasso_model.predict(X_test)

# Evaluate the model
lasso_mse = mean_squared_error(y_test, y_pred_lasso)
lasso_r2 = r2_score(y_test, y_pred_lasso)

print("Lasso Regression MSE:", lasso_mse)
print("Lasso Regression R-squared:", lasso_r2)

# Define the model
ridge_model = Ridge()

# Create a range of alpha values to test
param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}

# Set up the GridSearchCV
ridge_grid_search = GridSearchCV(ridge_model, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)

# Fit the model
ridge_grid_search.fit(X_train, y_train)

# Get the best parameters and model
best_ridge_alpha = ridge_grid_search.best_params_['alpha']
best_ridge_model = ridge_grid_search.best_estimator_

print("Best Ridge Alpha:", best_ridge_alpha)

# Evaluate the best model
y_pred_ridge_best = best_ridge_model.predict(X_test)
ridge_best_mse = mean_squared_error(y_test, y_pred_ridge_best)
ridge_best_r2 = r2_score(y_test, y_pred_ridge_best)

print("Best Ridge Regression MSE:", ridge_best_mse)
print("Best Ridge Regression R-squared:", ridge_best_r2)

# Define the range of alpha values to test
alphas = np.logspace(-4, 4, 100)

# Create and fit the LassoCV model
lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=42)
lasso_cv.fit(X_train, y_train)

# Get the best alpha value
best_lasso_alpha = lasso_cv.alpha_
print("Best Lasso Alpha:", best_lasso_alpha)

# Evaluate the best model
y_pred_lasso_best = lasso_cv.predict(X_test)
lasso_best_mse = mean_squared_error(y_test, y_pred_lasso_best)
lasso_best_r2 = r2_score(y_test, y_pred_lasso_best)

print("Best Lasso Regression MSE:", lasso_best_mse)
print("Best Lasso Regression R-squared:", lasso_best_r2)

# Create polynomial features
degree = 2  # You can adjust the degree based on your needs
poly = PolynomialFeatures(degree=degree)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)
# Fit the model
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Predict on the test set
y_pred_poly = model.predict(X_test_poly)

# Fit the model
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Predict on the test set
y_pred_poly = model.predict(X_test_poly)

# Evaluate the model
poly_mse = mean_squared_error(y_test, y_pred_poly)
poly_r2 = r2_score(y_test, y_pred_poly)

print("Polynomial Regression MSE:", poly_mse)
print("Polynomial Regression R-squared:", poly_r2)

# Assuming y_test and y_pred_poly are your actual and predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_poly, alpha=0.5)
plt.xlabel("Actual Fuel Consumption")
plt.ylabel("Predicted Fuel Consumption")
plt.title("Actual vs. Predicted Fuel Consumption (Polynomial Regression)")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.show()



from sklearn.metrics import mean_squared_error, r2_score

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

import xgboost as xgb
model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

from sklearn.svm import SVR
model = SVR(kernel='rbf', C=1.0, epsilon=0.1)  # Try different kernels ('rbf', 'poly')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

from sklearn.neural_network import MLPRegressor

# Initialize the model
mlp_model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)

# Fit the model
mlp_model.fit(X_train, y_train)

# Make predictions
y_pred_mlp = mlp_model.predict(X_test)

# Evaluate the model
mse_mlp = mean_squared_error(y_test, y_pred_mlp)
r2_mlp = r2_score(y_test, y_pred_mlp)

print(f"MLP Regressor MSE: {mse_mlp}")
print(f"MLP Regressor R-squared: {r2_mlp}")



# Save the model to a file using joblib
model_path = 'DTR_model.joblib'
joblib.dump(model, model_path)

# The model is now saved as 'DTR_model.joblib'

